A. Installing the files that snapback needs

On Darwin (Mac OS-X), you'll need to use Homebrew to install a fairly-full set
of GNU utilities:

    sudo brew install bash gnu-sed pstree coreutils findutils ...

On BSD, you'll need to do something similar to the above, and set the PATH
inside /etc/snapback/configure.sh (see below) to put them first.

Grab the library that's needed, by running:

    git clone git@github.com:HRDAG/libsnap.git

in the parent directory of your snapback workspace.

From the root of the snapback workspace, copy-and-paste the following commands
into a terminal window running bash:

    pushd   ../libsnap/src/lockpid && make && popd &&
    sudo cp ../libsnap/src/lockpid/lockpid	/usr/local/bin/
    sudo cp ../libsnap/bin/libsnap.sh		/usr/local/bin/
    sudo cp bin/snapback			/usr/local/bin/
    sudo cp etc/sudoers.d/snapback		/etc/sudoers.d/

    sudo cp -r etc/snapback /etc/	# could instead copy to ~/etc/ or ...
    cd /etc/snapback &&			# ... /usr/local/etc/ (and cd to there)
    for fs in *.sample; do f=${fs%.sample}; sudo cp -v $fs $f; done &&
    echo "now edit the files copied from the .sample files"


B. TL;DR (Too Long; Didn't Read)

To get up and running quickly, here's the minimal edits to
/etc/snapback/configure.sh that are needed for some sample configurations:

   1. single type of backup on flash drive(s)

      Set is_multi_writer_drive=$true

      You can name your drives A, B, C, etc.

   2. single type of backup on hard disk(s)

      If you have an LVM VG on an SSD (flash drive), set an FS_log_VG= value.

      You can name your drives A, B, C, etc.

   3. backups split across a pair of (small?) flash drives

      Set is_multi_writer_drive=$true

      Here's some sample naming for the drives:

      H (hourly) drive (smaller?), frequent backups: create
      drv_name2months_per_span_for_day_prune[H]=0 (delete old snapshots).
      Could use e.g. drv_name2dirs_to_backup[H]=/home if drive is small.

      D (daily) drive, long-term backups:
      create drv_name2backup_period[D]=24

      If you want to have multiple H and D drives (so some drives can be
      off-site), then name them H1, H2, D1, D2, etc.  A customization for H
      (e.g. drv_name2backup_period[H]=1) is used for any H## (e.g. H1 and H2).

On GNU/Linux, run "sudo snapback mkfs ..."  to build a filesystem on each
drive (which will *DESTROY* any existing data on the specified partition).

On Darwin (macOS aka Mac OS-X), use an APFS filesystem labeled backup-{name},
and set backup_dir_prefix=/Volumes/backup- in /etc/snapback/configure.sh .


C. What filesystem type(s) should I use?

WARNING: if you heavily use an application which generates lots of hard-linked
files (like HRDAG's 'snap' aka 'snapshot'; or Red Hat's 'yum' package
manager's /var/lib/yum/yumdb/, which we exclude by default), you could run
into the "Too many links" rsync error if you use a backup-drive
filesystem-type that has too-small limits on the maximum number of hard-links
per inode (when mounting NTFS via 'fuseblk', the max links is 8K; for ext4,
the max is 65K).  In this case, you'll ideally use a filesystem like XFS (or
Btrfs, if you're adventuresome) that uses a 32-bit word to hold the inode's
link reference-count; similarly, APFS on Darwin (macOS aka Mac OS-X) uses a
32-bit link-count (*never* use HFS: every "hard link" creates a hidden copy of
the file?!).

My experience is that ext4 is faster than XFS (especially when filesystems get
above 55% or 65% or 75% blocks used), _except_ when the drive is an SSD.  ext4
is less fragile (an XFS crash resulted in most files being moved to
/lost+found/); and xfs_repair wants to use an immense amount of RAM, besides
taking a long time.  But I use XFS anyway, because my filesystems have a huge
number of hardlinks (because we use HRDAG's 'snap' aka 'snapshot').


D. How do I encrypt a backup drive?

On a Mac, use the macOS encryption utility.

On GNU/Linux (and probably on *BSD), you can use HRDAG's 'snapcrypt' utility.
Here's how to install it:

    git clone git@github.com:HRDAG/snapcrypt.git
    cd snapcrypt
    sudo cp sbin/snapback			/usr/local/sbin/
    sudo cp -r etc/snapcrypt /etc/	# could instead copy to ~/etc/ or ...
    cd /etc/snapcrypt &&		# ... /usr/local/etc/ (and cd to there)
    for fs in *.sample; do f=${fs%.sample}; sudo cp -v $fs $f; done &&
    echo "now edit the files copied from the .sample files"

Find the drive's device name with:
    ls -lt /dev/sd*[^0-9]	# most-recently-plugged-in-drive at the top

Save the device name, and specify the drive-name you will use
    raw_device=/dev/sd?  drive_name=?

Verify it's the drive you want:
    sudo fdisk -l ${raw_device%%[0-9]*}

Choose the partition you want:
    raw_device=${raw_device}?	# you could instead encrypt the whole drive

See if someone is using it:
    sudo fuser -m -v $raw_device*

Create an encryption keypair (if don't already have one you want to use):
    keypair=/etc/snapcrypt/keys/keyfile.bin
    sudo mkdir -p $(dirname $keypair)
    cd $(dirname $keypair)
    # create key $(basename $keypair)

Encrypt the drive, then create a decrypted mapping:
    # If you're going to use ext4 with snapback's default block size (-b 1024),
    # then you'll need to encrypt the drive with the matching sector size:
	snapcrypt init -s 1024 $drive_name $raw_device

    # If you'll instead use XFS, which defaults to a block size of 4096,
    # you can use snapcrypt's default sector size:
	snapcrypt init $drive_name $raw_device

    # If it fails and syslog says "Device size is not multiple of sector_size",
    # you have options (some of which WILL *DESTROY* all the drive's data):

    # 0. easiest, but least efficient: re-run 'init' with the smallest
    # sector-size that will work, i.e. copy-and-paste the following 3 lines:
         snapcrypt init -s 2048 $drive_name $raw_device ||
         snapcrypt init -s 1024 $drive_name $raw_device ||
         snapcrypt init -s  512 $drive_name $raw_device

    # 1. DESTROY data: if the drive has no needed data, then run
    # ' fdisk ${raw_device%%[0-9]*} ', run 'g' to create a new GPT disklabel,
    # then run 'n' to create a new partition, and select all the defaults,
    # and remember the reported size; run 'd' & 'n' to re-create partition 1,
    # and for the "Last sector, ... or +size{...}" use +size, e.g. +119.5G
    # then run 'w' to write the results and DESTROY what's on the drive

    # 2. safely retain data: to use the *last* partition on the drive, run
    # ' fdisk ${raw_device%%[0-9]*} ', then delete the *last* partition,
    # then run 'n' to create a new partition, and select all the defaults,
    # and remember the reported size; run 'd' & 'n' to re-create partition,
    # and for the "Last sector, ... or +size{...}" use +size, e.g. +119.5G
    # then run 'w' to write the results

    # 3. gently redo early partition: to use an earlier partition, run
    # ' fdisk ${raw_device%%[0-9]*} ', then delete your partition, then
    # run 'n' & '#' to create a new partition, and select all the defaults,
    # and remember the reported size; run 'd' & 'n' & # to re-create partition,
    # and for the "Last sector, ... or +size{...}" use +size, *BUT* specify
    # a size that's slightly smaller than the remembered size, e.g. +119.4G
    # then run 'w' to write the results

    snapcrypt will have created a udev rule for the device, test it with:

      udevadm test $(udevadm info $raw_device | sed -n 's/^P: //p') |& tail -n1


E. What filesystem parameters should I use?

If you have existing ext4 backup drives that are no bigger, use 'snapback
copy-drive' to initialize the new drive; otherwise, use 'snapback mkfs' to
create a filesystem, and 'snapback update-drive' to copy snapshots from your
other backup drives.

To determine your actual average directory size, e.g. to optimize ext4:
     # Specify your filesystem type with mkfs_cmd_opts= in configure.sh .
     # Choose the fastest drive (especially SSD/flash) that holds 1 backup.
     snapback mkfs -f -b 1024 {device} {name} # first test, use smallest -b
     # Configure dirs_to_backup= in configure.sh, and exclude.txt .
     snapback backup-drive {name}
     snapback dir-sizes {device} {name}

     snapback mkfs -f -b 2048 {drive-name} # next test, overwrite older FS
     ...

If your filesystem stores small dirs in the inode (as with XFS), you might
mess with isize.

NOTE: the rest of this document is heavily oriented towards a GNU/Linux system
like Ubuntu, with partitions holding e.g. ext4 filesystems.  [The 'mkfs' and
'copy-drive' actions (see below) support XFS filesystems, because HRDAG uses
'snap' aka 'snapshot', so we experience "Too many links" errors with our 3
years of backup snapshots on an ext4 filesystem.]


F. Re-using existing backup drives and partitions

Unless you have drive partitions with one or more existing backup snaphots,
you can skip this whole section.

If you have drive partitions with one or more existing snaphots, it's best to
use "cp -al" to copy all the snapshots onto one partition (ideally, the
smallest partition that will hold all the fully-hard-linked files ...).
Before each individual "cp -al", hard-link all the identical files on the
source (which will decrease disk space) using e.g. 'rmlint'
(https://github.com/sahib/rmlint); after each "cp -al", run 'rmlint' on the
whole destination partition.

Once all the snapshots have been copied to one partition (which I'll call the
master partition), you need to rename the snapshots to conform to snapback's
standards, and make sure all the snapshots are in the root of the partition
(and, ideally, nothing else is in the root of the partition, except perhaps in
an e.g. /.my-stuff/ sub-directory).  To see snapback's snapshot naming
conventions, search for 'snapshot_glob=' in snapback, and read the preceding
comment.

Now, label your master partition.  First, to see a good fstab record, run e.g.:

    snapback -d mkfs -f $device A	# -d means Debug i.e. "simulation"

which will also indicate the label to use with e.g. 'e2label'.

If you weren't able to use your smallest partition as the master, but the
amount of disk space actually in-use on the master will fit in smaller backup
partitions, run e.g. (for ext*):

    device=/dev/xxxx
    sudo umount $device
    sudo resize2fs -P $device

to see if it can be shrunk enough to fit on a smaller partition.  If so, run:

    nohup sudo resize2fs -M -p $device & sleep 0.1; tail -f nohup.out

When it finishes (it could take hours), again use "resize2fs -P" to see if
it's shrunk enough to fit in any of your smaller partitions.

Before you copy the master partition to your other partitions, you may want to
think seriously about first encrypting the other partitions: See section D.

Now you can use "snapback copy-drive" (which will show you what arguments are
needed) to copy your master partition to each of the partitions large enough
to hold its contents.  If the partitions are physically smaller than the
master partition, "snapback copy-drive" runs a 'dd' command that will fail;
but you can run a simulation with "snapback -d copy-drive {args}" to see what
commands you need to run to finish the job (it's best to 'nohup' the 'fsck'
command, it will take a long time).  You can copy multiple partitions
simultaneously (this is desired, since all the copies will share the same
source contents in the page cache).  You don't need to use the 'nohup' command
with "snapback copy-drive": it runs 'nohup' internally, and saves the output
to $FS_label.out .

If you copied your master partition to encrypted partitions, you now want to
unmap your master partition and use "snapback copy-drive" to copy the
smallest partition back to your now-encrypted no-longer-master partition.

Now you can mount all your backup partitions on $drive_dir_prefix (defined in
configure.sh).


G. Configuring 'snapback'

All configuration takes place in

    /etc/snapback/configure.sh		# or in ~/etc/ or /usr/local/etc/
    /etc/snapback/exclude.txt		# ditto

NOTE: From here on, when I mention "configure.sh" or "exclude.txt", I mean the
file in ~/etc/snapack/ else /usr/local/etc/snapback/ else /etc/snapback/ .

Some of the configure.sh variables are quite abstract, you'll need to
carefully read the comments.  And it's probably best to leave the tuning
variables alone (see 'K. snapback tuning'), and re-read those variable's
comments after you've let cron create a lot of snapshots and you start to run
out of resources on some drives.

[If there's something you want to change that can't be accomplished by editing
those two files, email sweikart@gmail.com with a description of what you want
to accomplish.]

configure.sh can hold functions that are assigned to hook variables, so
snapback can call your own functions.  For example:

1. set_drive_name_pre_hook: you can set set_drive_name_pre_hook= to the name
of a function that can munge the 'name' variable before it's checked and
processed.

2. set_drive_name_post_hook: you can set set_drive_name_post_hook= to the
name of a function that can munge the 'drive_name' variable before it's used
to create various directories (for a new backup drive).

3. backup_drive_pre_hook: you can set backup_drive_pre_hook= to the name of a
function that's called before a backup's rsync, to e.g. create an
LVM/Btrfs/ZFS/etc snapshot (that's specified in $dirs_to_backup) of the source
filesystem, and/or do a database dump to store in the backup snapshot.

4. backup_drive_post_hook: you can set backup_drive_post_hook= to the name of
a function that's called after the backup's rsync, to e.g. delete the
source-filesystem snapshot that's specified in $dirs_to_backup, and/or delete
a database-dump, and/or write the snapshot's name to a database (depending on
the value of $status).

5. rm_snapshot_pre_hook: you can set rm_snapshot_pre_hook= to the name of a
function that's called before a .rm snapshot is deleted: you can prevent the
deletion by setting snapshot to the null string (and removing the snapshot's
.rm suffix); you can prevent future pruning by touch'ing .keep in the snapshot.

6. rm_snapshot_post_hook: you can set rm_snapshot_post_hook= to the name of a
function that's called after a .rm snapshot is deleted, e.g. to delete the
existence of the snapshot from a database.


H. Preparing brand new partitions when you have no existing snapshots

You can use:

    sudo snapback mkfs -f

to prepare a new partition.  Unless your setup auto-mounts partitions
(e.g. Darwin aka MacOS, or users of 'snapcrypt'), add a record to /etc/fstab
(or equivalent), then mount the partition on $drive_dir_prefix (defined in
configure.sh).

If this is your first snapback drive, create a backup by running:

    sudo snapback backup-drive {drive-name}

which will create a backgrounded job and immediately return.  You can then
watch the backup by running:

    snapback watch

See doc/watch.txt for details on using the 'watch' action.

If you already have an existing snapback drive, you should instead copy its
latest backup to the new drive:

   sudo \
   nohup snapback copy-snapshot -B /backup/{old-drive-name}/latest {drive-name}

Once you've created and mounted all the partitions you plan to use, you can
backup all of them simultaneously with:

    sudo snapback backup-drives

but it's best to let 'cron' do it ...


I. Automation with cron

Add your backup drive_names to drive_name_regex= in configure.sh .

NOTE: send sweikart@gmail.com instructions on how to support automation if the
following doesn't work.  [On macOS, see Library/***]

Once everything looks OK, from the root of the snapback workspace,
copy-and-paste the following command into a terminal window:

    sudo cp etc/cron.d/snapback /etc/cron.d/

If you get the error:

    cp: cannot create regular file `/etc/cron.d//': Not a directory

then you'll have to copy the snapback command (under command-to-be-executed)
from etc/cron.d/snapback into a file in /etc/cron.hourly/ . If you don't have
/etc/cron.hourly/ , then you'll probably need to append the last line of
etc/cron.d/snapback to your /etc/crontab file.


J. OS tuning

To devote more RAM to inode and directory caches with the Linux kernel,
decrease the value of vm.vfs_cache_pressure.  You might start with a value
near 20; if your filesytem drives are SSD but your backup drives are
rotational disks, a lower value might be safe for regular work while being
good for backups.  But there are some details ...

Here's the (very old) linux kernel documentation on vm.vfs_cache_pressure:

    vfs_cache_pressure
    ------------------

    This percentage value controls the tendency of the kernel to reclaim
    the memory which is used for caching of directory and inode objects.

    At the default value of vfs_cache_pressure=100 the kernel will attempt to
    reclaim dentries and inodes at a "fair" rate with respect to pagecache and
    swapcache reclaim.  Decreasing vfs_cache_pressure causes the kernel to
    prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the
    kernel will never reclaim dentries and inodes due to memory pressure and
    this can easily lead to out-of-memory conditions. Increasing
    vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim
    dentries and inodes.

    Increasing vfs_cache_pressure significantly beyond 100 may have negative
    performance impact. Reclaim code needs to take various locks to find
    freeable directory and inode objects. With vfs_cache_pressure=1000, it
    will look for ten times more freeable objects than there are.

The snapback repo includes a file to change the default:

    etc/sysctl.d/60-snapback.conf

Then you want to run the 'pmie' command, typically by installing the 'pcp'
package.

If you make vm.vfs_cache_pressure too low, pmie will write a message to syslog
tht that looks like:

    pcp-pmie[PID]: Severe demand for real memory N.Mpgsout/s@HOST

If you installed /etc/cron.d/snapback, every 5 minutes snapback will search
syslog for these messages (along with other error messages); if you run
'snapback watch', you'll see all the error messages that were recently found.
If you set max_swapout_MBps= in configure.sh, 'snapback watch' will report the
percentage of swap bandwidth that's used.  Here's how to determine the value
of max_swapout_MBps:

    IfRun=echo		# comment-out to not debug

    swap_dev=/dev/???		# from swapon -s
    swap_label=????		# from LABEL=???? in /etc/fstab

    $IfRun swapoff $swap_dev &&
    $IfRun dd if=/dev/zero of=$swap_dev bs=1M count=20480

    $IfRun mkswap --label $swap_label $swap_dev &&
    $IfRun swapon -a &&
    $IfRun swapon --show

So, first assess how often this happens with the default value of
vm.vfs_cache_pressure, by checking all the older syslog files:

    snapback big-swapouts

Now you can experiment with different values for vm.vfs_cache_pressure, and
then check to see if things got worse:

    snapback watch

which will find errors in yesterday's (if /etc/logrotate.conf contains
'dateext' and 'delaycompress') and today's syslog files.


K. snapback tuning

There's two kinds of problems that will require tuning: you're running out
of drive space or inodes, or you're getting "Too many links" errors.  If you
get these errors when "snapback watch" reports e.g.

    Number pruned snapshots awaiting 'rm':  A=0    C=0    G=0

then you'll need to edit configure.sh and either increase '*backup_period',
and/or decrease one of the four '*_per_span_for_*_prune' variables, and/or
decrease pruning_span_scale_factor; carefully read the comments to figure out
what these do.

[If you'd rather that these variables were automagically tuned by snapback
itself (with some tuning-priorities specified by you), ask sweikart@gmail.com
to implement something like the ideas in doc/extra-prune.txt .]

After you change a *prun* variables, measure the impact with:

    snapback predict-pruning [-f span] name(s)

where the optional "-f span" specifies the Final span that will be pruned
(e.g. with "-f hour", stop pruning after the "hour" span).  This will show and
count all the snapshots that would be pruned, including the ones that have
already been renamed to *.rm (and thus could be partially deleted).

Do *not* change variables close to the 5 minute mark, because that's when cron
will kick off a prune, which will rename the to-be-pruned snapshots to
Date,Hour.rm, and then start deleting them (but, if you set
'is_multi_writer_drive=$false' because you're using disk drives not SSDs, then
the prune won't be started until the backup finishes).

Soon after cron kicks of a prune, "snapback watch" will show that your drive
now has non-0 "Number pruned .rm snaps awaiting rm -r"; you'll need to wait
until this value goes to 0, then decide if your tuning-change freed up enough
disk space.  See doc/watch.txt for more details on watching how the system
responds to tuning.

If you indvertently kick off a too-aggressive prune on multiple drives, you
can recover with:

    snapback k p all			# kill all prune jobs
    snapback undelete /backup/*/*.rm	# rename *.rm snapshots to *.partial
    snapback update-drive all		# restore *.partial from other drives

Remember, you don't need to change the global tuning variables, you can set a
custom value for each partition that's running out of resources.  To see
examples of this, search configure.sh for '[Z]', to see how very-small values
are used for the pseudo-partition named 'Z' ('Z' is used for regression tests,
and for studying how pruning works with the "-T 5" option to snapback), or
search configure.sh for '[X]' (sample customizations).

--

Instead of the above, you could edit exclude.txt (in same directory as
configure.sh) to exclude pieces of your system.  Again, you'll need to wait
for the next prune job to start, and then wait until the drive's log
(/var/log/snapback/drives/{drive-name}/messages.log) reports:

    pruned exclude-patterns from _all_ snapshots

And note that you can use the source_drive_specific_config_file function at
the end of configure.sh.sample to have separate exclude files for different
categories of partitions (BUT, if you do this, you won't be able to
update-drive between those categories).

Note that you might be able to accomplish the same goal by changing
'dirs_to_backup' in configure.sh, which is also convenient because it's easy
to add separate per-drive values to configure.sh .  BUT, 'snapback' won't
currently delete the no-longer-backed-up-dirs from older snapshots; also,
update-drives will only copy between drives that have the same values for
dirs_to_backup .
